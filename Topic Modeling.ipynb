{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Libraries</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#preprocessing-functions\" data-toc-modified-id=\"preprocessing-functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>preprocessing functions</a></span></li><li><span><a href=\"#example-preprocessed-document\" data-toc-modified-id=\"example-preprocessed-document-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>example preprocessed document</a></span></li></ul></li><li><span><a href=\"#grab-a-random-document\" data-toc-modified-id=\"grab-a-random-document-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>grab a random document</a></span><ul class=\"toc-item\"><li><span><a href=\"#preprocess-the-documents\" data-toc-modified-id=\"preprocess-the-documents-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>preprocess the documents</a></span></li></ul></li><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bag of Words</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dictionary\" data-toc-modified-id=\"Dictionary-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Dictionary</a></span></li><li><span><a href=\"#Remove-Extremes\" data-toc-modified-id=\"Remove-Extremes-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Remove Extremes</a></span></li><li><span><a href=\"#BoW\" data-toc-modified-id=\"BoW-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>BoW</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**\n",
    "\n",
    "LDA can be used for topic modeling or document classification.  It utilizes two dirichlet distributions, one as a topic per document model and one as a words per topic model.  Documents are modeled as multinomial distribution of topics, with each topic being modeled as a multinomial distribution of words.\n",
    "\n",
    "Some assumptions:\n",
    "- every chunk of text will contain words that are somehow related\n",
    "- documents are produced from a mixture of topics\n",
    "- topics generate words based on their multinomial distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# data cleaning\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The dataset we'll use is a list of over one million news headlines published over a period of 15 years. We'll start by loading it from the `abcnews-date-text.csv` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('data', 'abcnews-date-text.csv'), error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103660</th>\n",
       "      <td>20171231</td>\n",
       "      <td>the ashes smiths warners near miss liven up bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103661</th>\n",
       "      <td>20171231</td>\n",
       "      <td>timelapse: brisbanes new year fireworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103662</th>\n",
       "      <td>20171231</td>\n",
       "      <td>what 2017 meant to the kids of australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103663</th>\n",
       "      <td>20171231</td>\n",
       "      <td>what the papodopoulos meeting may mean for ausus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103664</th>\n",
       "      <td>20171231</td>\n",
       "      <td>who is george papadopoulos the former trump ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1103665 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1103660      20171231  the ashes smiths warners near miss liven up bo...\n",
       "1103661      20171231            timelapse: brisbanes new year fireworks\n",
       "1103662      20171231           what 2017 meant to the kids of australia\n",
       "1103663      20171231   what the papodopoulos meeting may mean for ausus\n",
       "1103664      20171231  who is george papadopoulos the former trump ca...\n",
       "\n",
       "[1103665 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab just the headlines for the first 300000 entries\n",
    "documents = data[:300000][['headline_text']];\n",
    "documents['index'] = documents.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "data cleaning will consist of:\n",
    "- Tokenization\n",
    "- Case standardization\n",
    "- Punctuation removal\n",
    "- Removal of words shorter than 3 characters\n",
    "- Stopword removal\n",
    "- Lemmatization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10061] No\n",
      "[nltk_data]     connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify wordnet is up-to-date for lemmatizing\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    \n",
    "    lem = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stem = stemmer.stem(lem)\n",
    "    \n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    # use gensim to tokenize the text & perform basic preprocessing\n",
    "    # converts to lower case, ignores tokens that are too short\n",
    "    tokens = gensim.utils.simple_preprocess(text, min_len=3)\n",
    "    \n",
    "    # initialize a list to store the result\n",
    "    result = []\n",
    "\n",
    "    # loop over the tokens\n",
    "    for token in tokens: \n",
    "        \n",
    "        # only keep if not a stop word\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            \n",
    "            # add the lemmatized & stemmed word to the result list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['mining', 'company', 'says', 'expansion', 'criticisms', 'are']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['mine', 'compani', 'say', 'expans', 'critic']\n"
     ]
    }
   ],
   "source": [
    "# grab a random document\n",
    "document_num = np.random.randint(len(documents))\n",
    "ex_doc = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in ex_doc.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(ex_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all the headlines\n",
    "processed_docs = documents[\"headline_text\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [aba, decid, communiti, broadcast, licenc]\n",
       "1                       [act, wit, awar, defam]\n",
       "2        [call, infrastructur, protect, summit]\n",
       "3         [air, staff, aust, strike, pay, rise]\n",
       "4     [air, strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to get a dictionary with all words and an integer ID\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aba\n",
      "1 broadcast\n",
      "2 communiti\n",
      "3 decid\n",
      "4 licenc\n",
      "5 act\n",
      "6 awar\n",
      "7 defam\n",
      "8 wit\n",
      "9 call\n",
      "10 infrastructur\n"
     ]
    }
   ],
   "source": [
    "# check out the first 10 dictionary entries\n",
    "\n",
    "i = 0\n",
    "\n",
    "for num, word in dictionary.iteritems():\n",
    "    \n",
    "    print(num, word)\n",
    "    i += 1\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the very rare (less than 15 docs) and very common (more than 10% of docs) words\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words model for each document\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(131, 1), (133, 1), (2024, 1), (3977, 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example document\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 138 (\"critic\") appears 1 time.\n",
      "Word 411 (\"say\") appears 1 time.\n",
      "Word 1343 (\"mine\") appears 1 time.\n",
      "Word 2558 (\"compani\") appears 1 time.\n",
      "Word 3402 (\"expans\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# example document\n",
    "for bow in bow_corpus[document_num]:\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow[0], \n",
    "                                                 dictionary[bow[0]], \n",
    "                                                 bow[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
